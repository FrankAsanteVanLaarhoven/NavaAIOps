"""
NavaFlow-VL-JEPA Production Inference Server with Ollama Integration

High-performance API service maintaining 0.15ms latency goal ("Ironclad")
with Ollama as the inference engine for maximum performance and control.
"""

import os
import sys
import subprocess
import asyncio
import uvicorn
import torch
import psutil
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from pathlib import Path
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", 8000))
MODEL_DIR = Path(os.getenv("MODEL_DIR", "./models"))
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# Ollama configuration
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_NUM_GPU = os.getenv("OLLAMA_NUM_GPU", "all")
OLLAMA_MAX_LOADED_MODELS = int(os.getenv("OLLAMA_MAX_LOADED_MODELS", "4"))

# --- API MODELS (Pydantic) ---

class GenerationRequest(BaseModel):
    """Request model for text generation"""
    prompt: str = Field(..., description="Input prompt for generation")
    model_id: str = Field(..., description="Model identifier (e.g., 'navajepa', 'llama3')")
    max_tokens: int = Field(default=2048, ge=1, le=8192)
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    stream: bool = Field(default=False, description="Enable streaming response")
    image_data: Optional[str] = Field(None, description="Base64 encoded image (for vision models)")

class VisionRequest(BaseModel):
    """Request model for vision-language inference"""
    image_data: str = Field(..., description="Base64 encoded image")
    text_query: str = Field(..., description="Text query about the image")
    model_id: str = Field(default="navajepa", description="Vision model identifier")

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    host_ip: str
    memory_usage_mb: float
    gpu_status: str
    gpu_memory_gb: Optional[float]
    loaded_models: List[str]
    ollama_available: bool

# --- MODEL MANAGER ---

class ModelManager:
    """Manages model discovery and loading"""
    
    def __init__(self, model_dir: Path):
        self.model_dir = model_dir
        self.models: Dict[str, Dict[str, Any]] = {}
        self._scan_models()
    
    def _scan_models(self):
        """Scan model directory for available models"""
        logger.info(f"Scanning models in {self.model_dir}")
        
        # Default model mappings
        default_models = {
            "navajepa": {
                "pth": self.model_dir / "navajepa_sota.pth",
                "gguf": self.model_dir / "navajepa_sota.gguf",
                "type": "vision-language"
            },
            "navajepa_v2": {
                "pth": self.model_dir / "navaflow_v2_version1a_checkpoint.pt",
                "gguf": self.model_dir / "navaflow_v2_version1a.gguf",
                "type": "vision-language"
            },
            "llama3": {
                "gguf": None,  # Ollama will handle this
                "type": "text"
            }
        }
        
        # Check which models actually exist
        for model_id, config in default_models.items():
            pth_path = config.get("pth")
            gguf_path = config.get("gguf")
            
            available_formats = []
            if pth_path and pth_path.exists():
                available_formats.append("pth")
            if gguf_path and gguf_path.exists():
                available_formats.append("gguf")
            if model_id == "llama3":
                available_formats.append("ollama")  # Ollama can pull it
            
            if available_formats:
                self.models[model_id] = {
                    **config,
                    "available_formats": available_formats,
                    "preferred_format": available_formats[0]  # Prefer GGUF for Ollama
                }
                logger.info(f"‚úÖ Found model: {model_id} (formats: {available_formats})")
    
    def get_model_path(self, model_id: str, format: Optional[str] = None) -> Optional[Path]:
        """Get path to model file"""
        if model_id not in self.models:
            return None
        
        config = self.models[model_id]
        preferred_format = format or config.get("preferred_format", "gguf")
        
        if preferred_format == "pth" and config.get("pth"):
            return config["pth"]
        elif preferred_format == "gguf" and config.get("gguf"):
            return config["gguf"]
        elif preferred_format == "pth" and config.get("pth"):
            return config["pth"]
        
        return None
    
    def is_available(self, model_id: str) -> bool:
        """Check if model is available"""
        return model_id in self.models
    
    def get_model_info(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Get model information"""
        if model_id not in self.models:
            return None
        return self.models[model_id]

# Initialize Model Manager
model_manager = ModelManager(MODEL_DIR)

# --- OLLAMA EXECUTOR ---

class OllamaExecutor:
    """Handles Ollama model execution"""
    
    def __init__(self, host: str = OLLAMA_HOST):
        self.host = host
        self.available = self._check_ollama_available()
    
    def _check_ollama_available(self) -> bool:
        """Check if Ollama is available"""
        try:
            import requests
            response = requests.get(f"{self.host}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            # Try command line check
            try:
                result = subprocess.run(
                    ["ollama", "list"],
                    capture_output=True,
                    timeout=2
                )
                return result.returncode == 0
            except:
                return False
    
    async def generate(
        self,
        model_id: str,
        prompt: str,
        stream: bool = False,
        max_tokens: int = 2048,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Generate text using Ollama"""
        if not self.available:
            raise HTTPException(
                status_code=503,
                detail="Ollama is not available. Please ensure Ollama is running."
            )
        
        # Check if model needs to be pulled
        model_path = model_manager.get_model_path(model_id, "gguf")
        
        # Use Ollama API if available
        try:
            import requests
            
            # For local GGUF files, we need to create a Modelfile
            if model_path and model_path.exists() and model_path.suffix == ".gguf":
                # Create a custom model name for this file
                custom_model_name = f"{model_id}_local"
                
                # Pull or use the model
                # In production, you'd register the GGUF file with Ollama
                ollama_model = custom_model_name
            else:
                # Use standard Ollama model name
                ollama_model = model_id
            
            # Make request to Ollama API
            payload = {
                "model": ollama_model,
                "prompt": prompt,
                "stream": stream,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": temperature
                }
            }
            
            response = requests.post(
                f"{self.host}/api/generate",
                json=payload,
                stream=stream,
                timeout=300
            )
            
            if stream:
                # Return streaming response
                def generate_stream():
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line)
                            yield json.dumps(data) + "\n"
                return {"stream": generate_stream()}
            else:
                result = response.json()
                return {
                    "text": result.get("response", ""),
                    "model": model_id,
                    "status": "success"
                }
        
        except ImportError:
            # Fallback to subprocess
            return await self._generate_subprocess(model_id, prompt, stream)
        except Exception as e:
            logger.error(f"Ollama generation error: {e}")
            raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")
    
    async def _generate_subprocess(
        self,
        model_id: str,
        prompt: str,
        stream: bool
    ) -> Dict[str, Any]:
        """Fallback: Generate using subprocess"""
        try:
            cmd = ["ollama", "run", model_id, prompt]
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                raise HTTPException(
                    status_code=500,
                    detail=f"Ollama execution failed: {stderr.decode()}"
                )
            
            return {
                "text": stdout.decode().strip(),
                "model": model_id,
                "status": "success"
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Subprocess error: {str(e)}")

# Initialize Ollama Executor
ollama_executor = OllamaExecutor()

# --- FASTAPI APP ---

app = FastAPI(
    title="NavaFlow AI OPS - Ollama Inference API",
    description="Production inference server with Ollama integration",
    version="2.0.0"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API ENDPOINTS ---

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "NavaFlow AI OPS - Ollama Inference Server",
        "version": "2.0.0",
        "status": "online",
        "ollama_available": ollama_executor.available
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    # Get memory usage
    memory = psutil.virtual_memory()
    memory_mb = memory.used / (1024 * 1024)
    
    # Get GPU info
    gpu_status = "Not Available"
    gpu_memory_gb = None
    if torch.cuda.is_available():
        gpu_status = "Available"
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
    
    # Get loaded models
    loaded_models = list(model_manager.models.keys())
    
    return HealthResponse(
        status="healthy" if ollama_executor.available else "degraded",
        host_ip=HOST,
        memory_usage_mb=round(memory_mb, 2),
        gpu_status=gpu_status,
        gpu_memory_gb=round(gpu_memory_gb, 3) if gpu_memory_gb else None,
        loaded_models=loaded_models,
        ollama_available=ollama_executor.available
    )

@app.get("/v1/models")
async def list_models():
    """List all available models"""
    models_info = []
    for model_id, config in model_manager.models.items():
        models_info.append({
            "id": model_id,
            "type": config.get("type", "unknown"),
            "formats": config.get("available_formats", []),
            "preferred_format": config.get("preferred_format", "unknown")
        })
    
    return {
        "models": models_info,
        "count": len(models_info)
    }

@app.post("/v1/generate")
async def generate_text(request: GenerationRequest):
    """
    Standard text generation endpoint using Ollama
    """
    # Validate model
    if not model_manager.is_available(request.model_id):
        raise HTTPException(
            status_code=404,
            detail=f"Model '{request.model_id}' not found. Available models: {list(model_manager.models.keys())}"
        )
    
    # Check Ollama availability
    if not ollama_executor.available:
        raise HTTPException(
            status_code=503,
            detail="Ollama is not available. Please start Ollama service."
        )
    
    try:
        # Generate using Ollama
        if request.stream:
            result = await ollama_executor.generate(
                model_id=request.model_id,
                prompt=request.prompt,
                stream=True,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            return StreamingResponse(
                result["stream"],
                media_type="application/x-ndjson"
            )
        else:
            result = await ollama_executor.generate(
                model_id=request.model_id,
                prompt=request.prompt,
                stream=False,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            return JSONResponse(content=result)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Generation error: {e}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/v1/vision")
async def predict_vision(request: VisionRequest):
    """
    Vision-language inference endpoint
    For NavaFlow-VL-JEPA models
    """
    # Validate model
    if not model_manager.is_available(request.model_id):
        raise HTTPException(
            status_code=404,
            detail=f"Model '{request.model_id}' not found"
        )
    
    # For vision models, we might need to use PyTorch directly
    # or convert to a format Ollama can handle
    model_info = model_manager.get_model_info(request.model_id)
    
    if model_info and model_info.get("type") == "vision-language":
        # Check if we have a PyTorch model
        pth_path = model_info.get("pth")
        
        if pth_path and pth_path.exists():
            # Use PyTorch inference (fallback to previous implementation)
            # In production, you'd load the actual model here
            return {
                "prediction": "Vision inference requires model loading",
                "world_state": {"prediction": "ON", "probability": 0.85},
                "action": {"predicted_action": "IDLE", "action_id": 0},
                "note": "Use /predict/vision from main.py for full PyTorch inference"
            }
    
    # Try Ollama if model supports it
    if ollama_executor.available:
        # For vision models in Ollama, we'd need to use a vision-capable model
        # This is a placeholder - actual implementation would handle vision inputs
        return {
            "error": "Vision inference via Ollama requires vision-capable model setup",
            "suggestion": "Use PyTorch inference endpoint for vision models"
        }
    
    raise HTTPException(
        status_code=501,
        detail="Vision inference not yet fully implemented for Ollama"
    )

@app.post("/v1/convert/check")
async def check_conversion_status(model_id: str):
    """Check if model needs conversion to GGUF"""
    if not model_manager.is_available(model_id):
        raise HTTPException(status_code=404, detail="Model not found")
    
    model_info = model_manager.get_model_info(model_id)
    pth_path = model_info.get("pth")
    gguf_path = model_info.get("gguf")
    
    needs_conversion = (
        pth_path and pth_path.exists() and
        (not gguf_path or not gguf_path.exists())
    )
    
    return {
        "model_id": model_id,
        "needs_conversion": needs_conversion,
        "pth_exists": pth_path.exists() if pth_path else False,
        "gguf_exists": gguf_path.exists() if gguf_path else False,
        "recommendation": "Convert to GGUF for optimal Ollama performance" if needs_conversion else "Model ready"
    }

# --- START SERVER ---

if __name__ == "__main__":
    print("=" * 60)
    print("üåç NavaFlow Production Server with Ollama Integration")
    print("=" * 60)
    print(f"üìç Host: {HOST}:{PORT}")
    print(f"üìÅ Model Directory: {MODEL_DIR}")
    print(f"ü§ñ Ollama Host: {OLLAMA_HOST}")
    print(f"‚úÖ Ollama Available: {ollama_executor.available}")
    print(f"üì¶ Available Models: {len(model_manager.models)}")
    print("=" * 60)
    
    uvicorn.run(app, host=HOST, port=PORT, log_level="info")
